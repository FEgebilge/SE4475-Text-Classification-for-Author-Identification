{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    torch.cuda.empty_cache()  # For compatibility if CUDA is used\n",
    "    gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "config = {\n",
    "    \"data_dir\": \"data\",\n",
    "    \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 15,\n",
    "    \"max_length\": 412,\n",
    "    \"num_folds\": 5,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"output_dir\": \"dbmdz/bert-base-turkish-cased-finetuned\",\n",
    "    # Learning rate parameters\n",
    "    \"initial_learning_rate\": 3e-5,\n",
    "    \"min_learning_rate\": 1e-5,\n",
    "    \"warmup_ratio\": 0.2,  \n",
    "    # Weight decay parameters\n",
    "    \"initial_weight_decay\": 0.01, \n",
    "    \"final_weight_decay\": 0.001,  \n",
    "    \"weight_decay_schedule\": \"linear\"  # Use linear decay\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Create necessary directories\n",
    "model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "os.makedirs(os.path.join(\"fold_metrics\", model_name), exist_ok=True)\n",
    "os.makedirs(os.path.join(\"plots\", model_name), exist_ok=True)\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize best metrics\n",
    "best_params = None\n",
    "best_f1 = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Load and Preprocess Dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1500 samples from data.\n",
      "Authors: {'AHMET ÇAKAR': 0, 'ALİ SİRMEN': 1, 'ATAOL BEHRAMOĞLU': 2, 'ATİLLA DORSAY': 3, 'AYKAN SEVER': 4, 'AZİZ ÜSTEL': 5, 'CAN ATAKLI': 6, 'DENİZ GÖKÇE': 7, 'EMRE KONGAR': 8, 'GÖZDE BEDELOĞLU': 9, 'HASAN PULUR': 10, 'HİKMET ÇETİNKAYA': 11, 'MEHMET ALİ BİRAND': 12, 'MEHMET DEMİRKOL': 13, 'MELTEM GÜRLE': 14, 'MERYEM KORAY': 15, 'MÜMTAZ SOYSAL': 16, 'NAZAN BEKİROĞLU': 17, 'NAZIM ALPMAN': 18, 'NEDİM HAZAR': 19, 'NEŞE YAŞIN': 20, 'OKAY KARACAN': 21, 'ÖZGE BAŞAK TANELİ': 22, 'REHA MUHTAR': 23, 'RIDVAN DİLMEN': 24, 'RUHAT MENGİ': 25, 'SELİM İLERİ': 26, 'TARHAN ERDEM': 27, 'UFUK BOZKIR': 28, 'YAŞAR SEYMAN': 29}\n",
      "Counter({0: 50, 1: 50, 2: 50, 3: 50, 4: 50, 5: 50, 6: 50, 7: 50, 8: 50, 9: 50, 10: 50, 11: 50, 12: 50, 13: 50, 14: 50, 15: 50, 16: 50, 17: 50, 18: 50, 19: 50, 20: 50, 21: 50, 22: 50, 23: 50, 24: 50, 25: 50, 26: 50, 27: 50, 28: 50, 29: 50})\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_dir):\n",
    "    texts, labels = [], []\n",
    "    authors = sorted([d for d in os.listdir(data_dir) if not d.startswith('.')])\n",
    "    author_to_label = {author: idx for idx, author in enumerate(authors)}\n",
    " \n",
    "    for author, label in author_to_label.items():\n",
    "        author_dir = os.path.join(data_dir, author)\n",
    "        if os.path.isdir(author_dir):\n",
    "            for file_name in os.listdir(author_dir):\n",
    "                if not file_name.endswith('.txt'):\n",
    "                    continue\n",
    "                file_path = os.path.join(author_dir, file_name)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                        texts.append(file.read())\n",
    "                        labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    return texts, labels, author_to_label\n",
    "\n",
    "# Load dataset\n",
    "texts, labels, author_to_label = load_dataset(config[\"data_dir\"])\n",
    "print(f\"Loaded {len(texts)} samples from {config['data_dir']}.\")\n",
    "print(f\"Authors: {author_to_label}\")\n",
    "print(Counter(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Data Augmentation ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Tokenization ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config[\"model_name\"],\n",
    "    num_labels=30,  # Number of authors\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "#print(model.config)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze().to(device),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze().to(device),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long).to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fold_metrics(true_labels, predictions, num_classes, fold):\n",
    "    \"\"\"Calculate and save metrics for each fold.\"\"\"\n",
    "    # Create model-specific directory\n",
    "    model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "    metrics_dir = os.path.join(\"fold_metrics\", model_name)\n",
    "\n",
    "    \n",
    "    # Calculate class-wise metrics\n",
    "    precision = precision_score(true_labels, predictions, average=None)\n",
    "    recall = recall_score(true_labels, predictions, average=None)\n",
    "    f1 = f1_score(true_labels, predictions, average=None)\n",
    "    \n",
    "    # Calculate averages\n",
    "    precision_avg = precision.mean()\n",
    "    recall_avg = recall.mean()\n",
    "    f1_avg = f1.mean()\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': [f'Class {i+1}' for i in range(num_classes)] + ['Average'],\n",
    "        'Precision': list(precision) + [precision_avg],\n",
    "        'Recall': list(recall) + [recall_avg],\n",
    "        'F1-Score': list(f1) + [f1_avg]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(metrics_dir, f\"performance_metrics_fold_{fold}.csv\")\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"Metrics for fold {fold} saved to {output_path}\")\n",
    "    \n",
    "    return precision_avg, recall_avg, f1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_plots(train_metrics, val_metrics, all_labels, all_preds, num_classes, fold):\n",
    "    # Create plots directory\n",
    "    model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "    plots_dir = os.path.join(\"plots\", model_name)\n",
    "    \n",
    "    # Extract metrics history\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    train_losses = [m['loss'] for m in train_metrics]\n",
    "    train_precisions = [m['precision'] for m in train_metrics]\n",
    "    train_recalls = [m['recall'] for m in train_metrics]\n",
    "    train_f1s = [m['f1'] for m in train_metrics]\n",
    "    \n",
    "    val_losses = [m['loss'] for m in val_metrics]\n",
    "    val_precisions = [m['precision'] for m in val_metrics]\n",
    "    val_recalls = [m['recall'] for m in val_metrics]\n",
    "    val_f1s = [m['f1'] for m in val_metrics]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = fig.add_gridspec(3, 2)\n",
    "    \n",
    "    # 1. Training and Validation Loss\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', marker='o')\n",
    "    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', marker='o')\n",
    "    ax1.set_title('Loss Progress')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. All Metrics Progress\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(epochs, train_precisions, 'b-', label='Train Precision', marker='o')\n",
    "    ax2.plot(epochs, train_recalls, 'g-', label='Train Recall', marker='o')\n",
    "    ax2.plot(epochs, train_f1s, 'r-', label='Train F1', marker='o')\n",
    "    ax2.plot(epochs, val_precisions, 'b--', label='Val Precision', marker='s')\n",
    "    ax2.plot(epochs, val_recalls, 'g--', label='Val Recall', marker='s')\n",
    "    ax2.plot(epochs, val_f1s, 'r--', label='Val F1', marker='s')\n",
    "    ax2.set_title('Metrics Progress')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Per-Class Performance\n",
    "    class_precision = precision_score(all_labels, all_preds, labels=np.arange(num_classes), average=None, zero_division=0)\n",
    "    class_recall = recall_score(all_labels, all_preds, labels=np.arange(num_classes), average=None, zero_division=0)\n",
    "    class_f1 = f1_score(all_labels, all_preds, labels=np.arange(num_classes), average=None, zero_division=0)\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Precision: {class_precision}\")\n",
    "    print(f\"Recall: {class_recall}\")\n",
    "    print(f\"F1-Score: {class_f1}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(num_classes)\n",
    "    width = 0.25\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.bar(x - width, class_precision, width, label='Precision', color='blue', alpha=0.7)\n",
    "    ax3.bar(x, class_recall, width, label='Recall', color='green', alpha=0.7)\n",
    "    ax3.bar(x + width, class_f1, width, label='F1-score', color='red', alpha=0.7)\n",
    "    \n",
    "    ax3.set_ylabel('Scores')\n",
    "    ax3.set_title('Per-Class Performance')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([f'Class {i+1}' for i in range(num_classes)], rotation=45)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # 4. Confusion Matrix\n",
    "    ax4 = fig.add_subplot(gs[2, :])  # Define ax4 here\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(f\"Confusion Matrix Shape: {cm.shape}\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f'Class {i+1}' for i in range(num_classes)])\n",
    "    disp.plot(ax=ax4, cmap='Blues', xticks_rotation=45)\n",
    "    ax4.set_title('Confusion Matrix')\n",
    "\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'combined_metrics_fold_{fold}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved to {os.path.join(plots_dir, f'combined_metrics_fold_{fold}.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, num_training_steps):\n",
    "    \"\"\"Create a learning rate scheduler with warmup and cosine decay.\"\"\"\n",
    "    num_warmup_steps = int(num_training_steps * config[\"warmup_ratio\"])\n",
    "    \n",
    "    # Print for debugging\n",
    "    print(f\"Total training steps: {num_training_steps}\")\n",
    "    print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "    print(f\"Initial lr: {config['initial_learning_rate']}\")\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=0.5\n",
    "    )\n",
    "\n",
    "def get_weight_decay(epoch, num_epochs):\n",
    "    \"\"\"Calculate weight decay based on training progress.\"\"\"\n",
    "    if config[\"weight_decay_schedule\"] == \"linear\":\n",
    "        progress = epoch / num_epochs\n",
    "        return config[\"initial_weight_decay\"] + (config[\"final_weight_decay\"] - config[\"initial_weight_decay\"]) * progress\n",
    "    else:  # cosine\n",
    "        progress = epoch / num_epochs\n",
    "        return config[\"initial_weight_decay\"] + (config[\"final_weight_decay\"] - config[\"initial_weight_decay\"]) * \\\n",
    "               (1 + math.cos(math.pi * progress)) / 2\n",
    "    \n",
    "def calculate_training_steps(train_dataloader, num_epochs):\n",
    "    return len(train_dataloader) * num_epochs // config[\"gradient_accumulation_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fold_metrics(true_labels, predictions, num_classes, fold):\n",
    "    \"\"\"Calculate and save metrics for each fold.\"\"\"\n",
    "    # Create model-specific directory\n",
    "    model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "    metrics_dir = os.path.join(\"fold_metrics\", model_name)\n",
    "\n",
    "    \n",
    "    # Calculate class-wise metrics\n",
    "    precision = precision_score(true_labels, predictions, average=None)\n",
    "    recall = recall_score(true_labels, predictions, average=None)\n",
    "    f1 = f1_score(true_labels, predictions, average=None)\n",
    "    \n",
    "    # Calculate averages\n",
    "    precision_avg = precision.mean()\n",
    "    recall_avg = recall.mean()\n",
    "    f1_avg = f1.mean()\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': [f'Class {i+1}' for i in range(num_classes)] + ['Average'],\n",
    "        'Precision': list(precision) + [precision_avg],\n",
    "        'Recall': list(recall) + [recall_avg],\n",
    "        'F1-Score': list(f1) + [f1_avg]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(metrics_dir, f\"performance_metrics_fold_{fold}.csv\")\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"Metrics for fold {fold} saved to {output_path}\")\n",
    "    \n",
    "    return precision_avg, recall_avg, f1_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Training Loop ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = AutoConfig.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "config_model.num_labels = 30\n",
    "config_model.hidden_dropout_prob = 0.25\n",
    "config_model.attention_probs_dropout_prob = 0.25\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, epoch_num, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", unit=\"batch\")\n",
    "    \n",
    "    # Update weight decay for this epoch\n",
    "    current_weight_decay = get_weight_decay(epoch_num, num_epochs)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['weight_decay'] = current_weight_decay\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions for metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar with current learning rate and weight decay\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{current_lr:.2e}',\n",
    "            'wd': f'{current_weight_decay:.2e}'\n",
    "        })\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'learning_rate': scheduler.get_last_lr()[0],\n",
    "        'weight_decay': current_weight_decay\n",
    "    }\n",
    "\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics including accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(model, train_data, train_labels, val_data, val_labels, fold):\n",
    "    # Create datasets and dataloaders\n",
    "    model = model.to(device)\n",
    "    clear_memory()\n",
    "    train_dataset = TextDataset(train_data, train_labels, tokenizer, config[\"max_length\"])\n",
    "    val_dataset = TextDataset(val_data, val_labels, tokenizer, config[\"max_length\"])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Calculate total steps and warmup steps\n",
    "    num_training_steps = len(train_loader) * config['num_epochs']\n",
    "    num_warmup_steps = int(num_training_steps * config['warmup_ratio'])\n",
    "    \n",
    "    # Initialize optimizer with initial learning rate and weight decay\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['initial_learning_rate'],\n",
    "        weight_decay=config['initial_weight_decay'],\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "   \n",
    "    \n",
    "    # Initialize learning rate scheduler with warmup and cosine decay\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    early_stopping_counter = 0\n",
    "    train_metrics_history = []\n",
    "    val_metrics_history = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Fold {fold}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total steps: {num_training_steps}, Warmup steps: {num_warmup_steps}\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Training\n",
    "        train_metrics = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device, \n",
    "            epoch, config['num_epochs']\n",
    "        )\n",
    "        train_metrics_history.append(train_metrics)\n",
    "        clear_memory()\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate_epoch(model, val_loader, device)\n",
    "        val_metrics_history.append(val_metrics)\n",
    "        clear_memory()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nTraining Metrics:\")\n",
    "        print(f\"Loss: {train_metrics['loss']:.4f}\")\n",
    "        print(f\"Precision: {train_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {train_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {train_metrics['f1']:.4f}\")\n",
    "        print(f\"Learning Rate: {train_metrics['learning_rate']:.2e}\")\n",
    "        print(f\"Weight Decay: {train_metrics['weight_decay']:.2e}\")\n",
    "        \n",
    "        print(f\"\\nValidation Metrics:\")\n",
    "        print(f\"Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            early_stopping_counter = 0\n",
    "            best_metrics = val_metrics\n",
    "            \n",
    "            # Save the best model for this fold\n",
    "            os.makedirs(config['output_dir'], exist_ok=True)\n",
    "            model_save_path = os.path.join(config['output_dir'], f'best_model_fold_{fold}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_f1': best_val_f1,\n",
    "                'config': config\n",
    "            }, model_save_path)\n",
    "            print(f\"\\nSaved best model for fold {fold} with F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "        if early_stopping_counter >= config['early_stopping_patience']:\n",
    "            print(\"\\nEarly stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Generate visualizations\n",
    "    generate_combined_plots(\n",
    "        train_metrics_history,\n",
    "        val_metrics_history,\n",
    "        best_metrics['true_labels'],\n",
    "        best_metrics['predictions'],\n",
    "        30,  # number of classes\n",
    "        fold\n",
    "    )\n",
    "    \n",
    "    # Save fold metrics\n",
    "    precision_avg, recall_avg, f1_avg = save_fold_metrics(\n",
    "        best_metrics['true_labels'],\n",
    "        best_metrics['predictions'],\n",
    "        30,\n",
    "        fold\n",
    "    )\n",
    "    \n",
    "    return best_metrics\n",
    "\n",
    "def cross_validate(texts, labels):\n",
    "    global best_f1\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config['num_folds'], shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        print(f\"\\nFold {fold + 1}/{config['num_folds']}\")\n",
    "        print(f\"Train size: {len(train_idx)}, Validation size: {len(val_idx)}\")\n",
    "        clear_memory()\n",
    "        \n",
    "        # Reset model for each fold\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config[\"model_name\"],\n",
    "            config=config_model\n",
    "        ).to(device)\n",
    "        \n",
    "        # Split data\n",
    "        X_train = [texts[i] for i in train_idx]\n",
    "        y_train = [labels[i] for i in train_idx]\n",
    "        X_val = [texts[i] for i in val_idx]\n",
    "        y_val = [labels[i] for i in val_idx]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        metrics = train_and_evaluate(\n",
    "            model, X_train, y_train, X_val, y_val, fold + 1\n",
    "        )\n",
    "        \n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'loss': metrics['loss'],\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1': metrics['f1']\n",
    "        })\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold + 1} Results:\")\n",
    "        print(f\"Loss: {metrics['loss']:.4f}\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        clear_memory()\n",
    "    \n",
    "    # Calculate average metrics across folds\n",
    "    avg_metrics = {\n",
    "        'loss': np.mean([m['loss'] for m in fold_metrics]),\n",
    "        'accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'recall': np.mean([m['recall'] for m in fold_metrics]),\n",
    "        'f1': np.mean([m['f1'] for m in fold_metrics])\n",
    "    }\n",
    "    \n",
    "    print('\\nAverage metrics across folds:')\n",
    "    print(f\"Loss: {avg_metrics['loss']:.4f}\")\n",
    "    print(f\"Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {avg_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {avg_metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {avg_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save overall results\n",
    "    results_df = pd.DataFrame(fold_metrics)\n",
    "    results_df.to_csv('fold_results.csv', index=False)\n",
    "    print(\"\\nSaved detailed fold results to 'fold_results.csv'\")\n",
    "    \n",
    "    best_f1 = avg_metrics['f1']\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Generate Overall Results ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_results(fold_metrics_dir, output_filename=\"overall_performance_metrics.csv\"):\n",
    "    # Gather all fold-level CSV files\n",
    "    fold_files = glob.glob(os.path.join(fold_metrics_dir, \"performance_metrics_fold_*.csv\"))\n",
    "    if not fold_files:\n",
    "        raise FileNotFoundError(\"No fold-level performance metrics files found in the directory.\")\n",
    "\n",
    "    # Initialize DataFrame for aggregation\n",
    "    all_folds_metrics = []\n",
    "\n",
    "    # Process each fold file\n",
    "    for file in fold_files:\n",
    "        fold_df = pd.read_csv(file)\n",
    "        all_folds_metrics.append(fold_df)\n",
    "\n",
    "    # Combine all fold data\n",
    "    combined_df = pd.concat(all_folds_metrics)\n",
    "\n",
    "    # Exclude the \"Average\" row for class-level aggregation\n",
    "    class_only_df = combined_df[~combined_df[\"Class\"].str.contains(\"Average\")]\n",
    "\n",
    "    # Aggregate metrics by class\n",
    "    aggregated_metrics = class_only_df.groupby(\"Class\").mean().reset_index()\n",
    "\n",
    "    # Sort the metrics by Class\n",
    "    aggregated_metrics[\"Class\"] = aggregated_metrics[\"Class\"].str.extract(r'(\\d+)').astype(int)\n",
    "    aggregated_metrics = aggregated_metrics.sort_values(by=\"Class\").reset_index(drop=True)\n",
    "\n",
    "    # Compute overall averages\n",
    "    overall_precision = aggregated_metrics[\"Precision\"].mean()\n",
    "    overall_recall = aggregated_metrics[\"Recall\"].mean()\n",
    "    overall_f1 = aggregated_metrics[\"F1-Score\"].mean()\n",
    "\n",
    "    # Add \"Average\" row to the results using pd.concat\n",
    "    average_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Class\": [\"Average\"],\n",
    "            \"Precision\": [overall_precision],\n",
    "            \"Recall\": [overall_recall],\n",
    "            \"F1-Score\": [overall_f1],\n",
    "        }\n",
    "    )\n",
    "    aggregated_metrics = pd.concat([aggregated_metrics, average_row], ignore_index=True)\n",
    "\n",
    "    # Save to a new CSV file\n",
    "    aggregated_metrics.to_csv(output_filename, index=False)\n",
    "    print(f\"Overall performance metrics saved to '{output_filename}'.\")\n",
    "\n",
    "    return aggregated_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_as_dataframe(aggregated_metrics):\n",
    "    \"\"\"\n",
    "    Display the overall performance metrics as a clean DataFrame.\n",
    "\n",
    "    Args:\n",
    "        aggregated_metrics (pd.DataFrame): DataFrame containing overall performance metrics.\n",
    "    \"\"\"\n",
    "    # Rename columns to match the teacher's format\n",
    "    aggregated_metrics = aggregated_metrics.rename(columns={\"Class\": \" \", \"Precision\": \"Precision\", \"Recall\": \"Recall\", \"F1-Score\": \"F-Score\"})\n",
    "    \n",
    "    # Display the DataFrame as is\n",
    "    display(aggregated_metrics.style.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n",
    "         {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
    "    ).set_caption(\"Overall Performance Metrics\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Execute Training ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Train size: 1200, Validation size: 300\n",
      "\n",
      "==================================================\n",
      "Training Fold 1\n",
      "==================================================\n",
      "Total steps: 4500, Warmup steps: 900\n",
      "\n",
      "Epoch 1/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [02:43<00:00,  1.83batch/s, loss=3.2635, lr=1.00e-05, wd=1.00e-02]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 3.4715\n",
      "Precision: 0.0442\n",
      "Recall: 0.0425\n",
      "F1-Score: 0.0315\n",
      "Learning Rate: 1.00e-05\n",
      "Weight Decay: 1.00e-02\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 3.3519\n",
      "Accuracy: 0.1233\n",
      "Precision: 0.1031\n",
      "Recall: 0.1233\n",
      "F1-Score: 0.0825\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.0825\n",
      "\n",
      "Epoch 2/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [02:41<00:00,  1.85batch/s, loss=2.8745, lr=2.00e-05, wd=9.40e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 3.1233\n",
      "Precision: 0.1710\n",
      "Recall: 0.1500\n",
      "F1-Score: 0.1402\n",
      "Learning Rate: 2.00e-05\n",
      "Weight Decay: 9.40e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 2.4431\n",
      "Accuracy: 0.3967\n",
      "Precision: 0.4638\n",
      "Recall: 0.3967\n",
      "F1-Score: 0.3537\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.3537\n",
      "\n",
      "Epoch 3/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [02:43<00:00,  1.84batch/s, loss=1.7473, lr=3.00e-05, wd=8.80e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 2.0597\n",
      "Precision: 0.5544\n",
      "Recall: 0.5200\n",
      "F1-Score: 0.5110\n",
      "Learning Rate: 3.00e-05\n",
      "Weight Decay: 8.80e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.5787\n",
      "Accuracy: 0.7133\n",
      "Precision: 0.7116\n",
      "Recall: 0.7133\n",
      "F1-Score: 0.6782\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.6782\n",
      "\n",
      "Epoch 4/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [02:41<00:00,  1.85batch/s, loss=1.2476, lr=2.95e-05, wd=8.20e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.2769\n",
      "Precision: 0.7877\n",
      "Recall: 0.7942\n",
      "F1-Score: 0.7873\n",
      "Learning Rate: 2.95e-05\n",
      "Weight Decay: 8.20e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0771\n",
      "Accuracy: 0.8500\n",
      "Precision: 0.8520\n",
      "Recall: 0.8500\n",
      "F1-Score: 0.8406\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.8406\n",
      "\n",
      "Epoch 5/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [02:41<00:00,  1.85batch/s, loss=0.8099, lr=2.80e-05, wd=7.60e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.9037\n",
      "Precision: 0.9052\n",
      "Recall: 0.9058\n",
      "F1-Score: 0.9046\n",
      "Learning Rate: 2.80e-05\n",
      "Weight Decay: 7.60e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9722\n",
      "Accuracy: 0.8567\n",
      "Precision: 0.8910\n",
      "Recall: 0.8567\n",
      "F1-Score: 0.8456\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.8456\n",
      "\n",
      "Epoch 6/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [02:42<00:00,  1.85batch/s, loss=0.6566, lr=2.56e-05, wd=7.00e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.7368\n",
      "Precision: 0.9785\n",
      "Recall: 0.9783\n",
      "F1-Score: 0.9782\n",
      "Learning Rate: 2.56e-05\n",
      "Weight Decay: 7.00e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9720\n",
      "Accuracy: 0.9000\n",
      "Precision: 0.9098\n",
      "Recall: 0.9000\n",
      "F1-Score: 0.8987\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.8987\n",
      "\n",
      "Epoch 7/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [02:41<00:00,  1.85batch/s, loss=0.6586, lr=2.25e-05, wd=6.40e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6888\n",
      "Precision: 0.9901\n",
      "Recall: 0.9900\n",
      "F1-Score: 0.9900\n",
      "Learning Rate: 2.25e-05\n",
      "Weight Decay: 6.40e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9071\n",
      "Accuracy: 0.9267\n",
      "Precision: 0.9319\n",
      "Recall: 0.9267\n",
      "F1-Score: 0.9258\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.9258\n",
      "\n",
      "Epoch 8/15\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 133/300 [01:12<01:30,  1.84batch/s, loss=0.6558, lr=2.09e-05, wd=5.80e-03]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Execute Cross-Validation with Hyperparameter Search ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 245\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(texts, labels)\u001b[0m\n\u001b[1;32m    242\u001b[0m y_val \u001b[38;5;241m=\u001b[39m [labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m val_idx]\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m fold_metrics\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m: fold \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    256\u001b[0m })\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Print fold results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 148\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_data, train_labels, val_data, val_labels, fold)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m train_metrics_history\u001b[38;5;241m.\u001b[39mappend(train_metrics)\n\u001b[1;32m    153\u001b[0m clear_memory()\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, device, epoch_num, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Gradient clipping for stability\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:34\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:216\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    214\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    215\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\u001b[0;32m--> 216\u001b[0m \u001b[43m_clip_grads_with_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:34\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:172\u001b[0m, in \u001b[0;36m_clip_grads_with_norm_\u001b[0;34m(parameters, max_norm, total_norm, foreach)\u001b[0m\n\u001b[1;32m    170\u001b[0m clip_coef_clamped_device \u001b[38;5;241m=\u001b[39m clip_coef_clamped\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads:\n\u001b[0;32m--> 172\u001b[0m     \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Execute Cross-Validation with Hyperparameter Search ---\n",
    "best_metrics = cross_validate(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Overall Results ---\n",
    "results_df = pd.DataFrame([{\n",
    "    'Loss': best_metrics['loss'],\n",
    "    'Accuracy': best_metrics['accuracy'],\n",
    "    'Precision': best_metrics['precision'],\n",
    "    'Recall': best_metrics['recall'],\n",
    "    'F1-Score': best_metrics['f1']\n",
    "}])\n",
    "results_df.to_csv(\"best_metrics.csv\", index=False)\n",
    "print(\"Results saved to 'best_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the final performance table as required\n",
    "overall_results = generate_overall_results(\"fold_metrics/dbmdz_bert-base-turkish-cased\", output_filename=\"overall_performance_metrics.csv\")\n",
    "display_as_dataframe(overall_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
