{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    torch.cuda.empty_cache()  # For compatibility if CUDA is used\n",
    "    gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "config = {\n",
    "    \"data_dir\": \"data\",\n",
    "    \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 10,\n",
    "    \"max_length\": 512, \n",
    "    \"num_folds\": 5,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"output_dir\": \"dbmdz/bert-base-turkish-cased-finetuned\",\n",
    "    # Learning rate parameters\n",
    "    \"initial_learning_rate\": 3e-5,\n",
    "    \"min_learning_rate\": 1e-5,\n",
    "    \"warmup_ratio\": 0.2,  \n",
    "    # Weight decay parameters\n",
    "    \"initial_weight_decay\": 0.01, \n",
    "    \"final_weight_decay\": 0.001,  \n",
    "    \"weight_decay_schedule\": \"linear\"  # Use linear decay\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Create necessary directories\n",
    "model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "os.makedirs(os.path.join(\"fold_metrics\", model_name), exist_ok=True)\n",
    "os.makedirs(os.path.join(\"plots\", model_name), exist_ok=True)\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize best metrics\n",
    "best_params = None\n",
    "best_f1 = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Load and Preprocess Dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1500 samples from data.\n",
      "Authors: {'AHMET ÇAKAR': 0, 'ALİ SİRMEN': 1, 'ATAOL BEHRAMOĞLU': 2, 'ATİLLA DORSAY': 3, 'AYKAN SEVER': 4, 'AZİZ ÜSTEL': 5, 'CAN ATAKLI': 6, 'DENİZ GÖKÇE': 7, 'EMRE KONGAR': 8, 'GÖZDE BEDELOĞLU': 9, 'HASAN PULUR': 10, 'HİKMET ÇETİNKAYA': 11, 'MEHMET ALİ BİRAND': 12, 'MEHMET DEMİRKOL': 13, 'MELTEM GÜRLE': 14, 'MERYEM KORAY': 15, 'MÜMTAZ SOYSAL': 16, 'NAZAN BEKİROĞLU': 17, 'NAZIM ALPMAN': 18, 'NEDİM HAZAR': 19, 'NEŞE YAŞIN': 20, 'OKAY KARACAN': 21, 'ÖZGE BAŞAK TANELİ': 22, 'REHA MUHTAR': 23, 'RIDVAN DİLMEN': 24, 'RUHAT MENGİ': 25, 'SELİM İLERİ': 26, 'TARHAN ERDEM': 27, 'UFUK BOZKIR': 28, 'YAŞAR SEYMAN': 29}\n",
      "Counter({0: 50, 1: 50, 2: 50, 3: 50, 4: 50, 5: 50, 6: 50, 7: 50, 8: 50, 9: 50, 10: 50, 11: 50, 12: 50, 13: 50, 14: 50, 15: 50, 16: 50, 17: 50, 18: 50, 19: 50, 20: 50, 21: 50, 22: 50, 23: 50, 24: 50, 25: 50, 26: 50, 27: 50, 28: 50, 29: 50})\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_dir):\n",
    "    texts, labels = [], []\n",
    "    authors = sorted([d for d in os.listdir(data_dir) if not d.startswith('.')])\n",
    "    author_to_label = {author: idx for idx, author in enumerate(authors)}\n",
    " \n",
    "    for author, label in author_to_label.items():\n",
    "        author_dir = os.path.join(data_dir, author)\n",
    "        if os.path.isdir(author_dir):\n",
    "            for file_name in os.listdir(author_dir):\n",
    "                if not file_name.endswith('.txt'):\n",
    "                    continue\n",
    "                file_path = os.path.join(author_dir, file_name)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                        texts.append(file.read())\n",
    "                        labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    return texts, labels, author_to_label\n",
    "\n",
    "# Load dataset\n",
    "texts, labels, author_to_label = load_dataset(config[\"data_dir\"])\n",
    "print(f\"Loaded {len(texts)} samples from {config['data_dir']}.\")\n",
    "print(f\"Authors: {author_to_label}\")\n",
    "print(Counter(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Data Augmentation ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Tokenization ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config[\"model_name\"],\n",
    "    num_labels=30,  # Number of authors\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "#print(model.config)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze().to(device),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze().to(device),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long).to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fold_metrics(true_labels, predictions, num_classes, fold):\n",
    "    \"\"\"Calculate and save metrics for each fold.\"\"\"\n",
    "    # Create model-specific directory\n",
    "    model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "    metrics_dir = os.path.join(\"fold_metrics\", model_name)\n",
    "\n",
    "    \n",
    "    # Calculate class-wise metrics\n",
    "    precision = precision_score(true_labels, predictions, average=None)\n",
    "    recall = recall_score(true_labels, predictions, average=None)\n",
    "    f1 = f1_score(true_labels, predictions, average=None)\n",
    "    \n",
    "    # Calculate averages\n",
    "    precision_avg = precision.mean()\n",
    "    recall_avg = recall.mean()\n",
    "    f1_avg = f1.mean()\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': [f'Class {i+1}' for i in range(num_classes)] + ['Average'],\n",
    "        'Precision': list(precision) + [precision_avg],\n",
    "        'Recall': list(recall) + [recall_avg],\n",
    "        'F1-Score': list(f1) + [f1_avg]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(metrics_dir, f\"performance_metrics_fold_{fold}.csv\")\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"Metrics for fold {fold} saved to {output_path}\")\n",
    "    \n",
    "    return precision_avg, recall_avg, f1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_plots(train_metrics, val_metrics, all_labels, all_preds, num_classes, fold):\n",
    "    # Create plots directory\n",
    "    model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "    plots_dir = os.path.join(\"plots\", model_name)\n",
    "    \n",
    "    # Extract metrics history\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    train_losses = [m['loss'] for m in train_metrics]\n",
    "    train_precisions = [m['precision'] for m in train_metrics]\n",
    "    train_recalls = [m['recall'] for m in train_metrics]\n",
    "    train_f1s = [m['f1'] for m in train_metrics]\n",
    "    \n",
    "    val_losses = [m['loss'] for m in val_metrics]\n",
    "    val_precisions = [m['precision'] for m in val_metrics]\n",
    "    val_recalls = [m['recall'] for m in val_metrics]\n",
    "    val_f1s = [m['f1'] for m in val_metrics]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = fig.add_gridspec(3, 2)\n",
    "    \n",
    "    # 1. Training and Validation Loss\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', marker='o')\n",
    "    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', marker='o')\n",
    "    ax1.set_title('Loss Progress')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. All Metrics Progress\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(epochs, train_precisions, 'b-', label='Train Precision', marker='o')\n",
    "    ax2.plot(epochs, train_recalls, 'g-', label='Train Recall', marker='o')\n",
    "    ax2.plot(epochs, train_f1s, 'r-', label='Train F1', marker='o')\n",
    "    ax2.plot(epochs, val_precisions, 'b--', label='Val Precision', marker='s')\n",
    "    ax2.plot(epochs, val_recalls, 'g--', label='Val Recall', marker='s')\n",
    "    ax2.plot(epochs, val_f1s, 'r--', label='Val F1', marker='s')\n",
    "    ax2.set_title('Metrics Progress')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Per-Class Performance\n",
    "    class_precision = precision_score(all_labels, all_preds, labels=np.arange(num_classes), average=None, zero_division=0)\n",
    "    class_recall = recall_score(all_labels, all_preds, labels=np.arange(num_classes), average=None, zero_division=0)\n",
    "    class_f1 = f1_score(all_labels, all_preds, labels=np.arange(num_classes), average=None, zero_division=0)\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Precision: {class_precision}\")\n",
    "    print(f\"Recall: {class_recall}\")\n",
    "    print(f\"F1-Score: {class_f1}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(num_classes)\n",
    "    width = 0.25\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.bar(x - width, class_precision, width, label='Precision', color='blue', alpha=0.7)\n",
    "    ax3.bar(x, class_recall, width, label='Recall', color='green', alpha=0.7)\n",
    "    ax3.bar(x + width, class_f1, width, label='F1-score', color='red', alpha=0.7)\n",
    "    \n",
    "    ax3.set_ylabel('Scores')\n",
    "    ax3.set_title('Per-Class Performance')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([f'Class {i+1}' for i in range(num_classes)], rotation=45)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # 4. Confusion Matrix\n",
    "    ax4 = fig.add_subplot(gs[2, :])  # Define ax4 here\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(f\"Confusion Matrix Shape: {cm.shape}\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f'Class {i+1}' for i in range(num_classes)])\n",
    "    disp.plot(ax=ax4, cmap='Blues', xticks_rotation=45)\n",
    "    ax4.set_title('Confusion Matrix')\n",
    "\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'combined_metrics_fold_{fold}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved to {os.path.join(plots_dir, f'combined_metrics_fold_{fold}.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, num_training_steps):\n",
    "    \"\"\"Create a learning rate scheduler with warmup and cosine decay.\"\"\"\n",
    "    num_warmup_steps = int(num_training_steps * config[\"warmup_ratio\"])\n",
    "    \n",
    "    # Print for debugging\n",
    "    print(f\"Total training steps: {num_training_steps}\")\n",
    "    print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "    print(f\"Initial lr: {config['initial_learning_rate']}\")\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=0.5\n",
    "    )\n",
    "\n",
    "def get_weight_decay(epoch, num_epochs):\n",
    "    \"\"\"Calculate weight decay based on training progress.\"\"\"\n",
    "    if config[\"weight_decay_schedule\"] == \"linear\":\n",
    "        progress = epoch / num_epochs\n",
    "        return config[\"initial_weight_decay\"] + (config[\"final_weight_decay\"] - config[\"initial_weight_decay\"]) * progress\n",
    "    else:  # cosine\n",
    "        progress = epoch / num_epochs\n",
    "        return config[\"initial_weight_decay\"] + (config[\"final_weight_decay\"] - config[\"initial_weight_decay\"]) * \\\n",
    "               (1 + math.cos(math.pi * progress)) / 2\n",
    "    \n",
    "def calculate_training_steps(train_dataloader, num_epochs):\n",
    "    return len(train_dataloader) * num_epochs // config[\"gradient_accumulation_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fold_metrics(true_labels, predictions, num_classes, fold):\n",
    "    \"\"\"Calculate and save metrics for each fold.\"\"\"\n",
    "    # Create model-specific directory\n",
    "    model_name = config[\"model_name\"].replace(\"/\", \"_\")\n",
    "    metrics_dir = os.path.join(\"fold_metrics\", model_name)\n",
    "\n",
    "    \n",
    "    # Calculate class-wise metrics\n",
    "    precision = precision_score(true_labels, predictions, average=None)\n",
    "    recall = recall_score(true_labels, predictions, average=None)\n",
    "    f1 = f1_score(true_labels, predictions, average=None)\n",
    "    \n",
    "    # Calculate averages\n",
    "    precision_avg = precision.mean()\n",
    "    recall_avg = recall.mean()\n",
    "    f1_avg = f1.mean()\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': [f'Class {i+1}' for i in range(num_classes)] + ['Average'],\n",
    "        'Precision': list(precision) + [precision_avg],\n",
    "        'Recall': list(recall) + [recall_avg],\n",
    "        'F1-Score': list(f1) + [f1_avg]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(metrics_dir, f\"performance_metrics_fold_{fold}.csv\")\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"Metrics for fold {fold} saved to {output_path}\")\n",
    "    \n",
    "    return precision_avg, recall_avg, f1_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Training Loop ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = AutoConfig.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "config_model.num_labels = 30\n",
    "config_model.hidden_dropout_prob = 0.2\n",
    "config_model.attention_probs_dropout_prob = 0.2\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, epoch_num, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", unit=\"batch\")\n",
    "    \n",
    "    # Update weight decay for this epoch\n",
    "    current_weight_decay = get_weight_decay(epoch_num, num_epochs)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['weight_decay'] = current_weight_decay\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions for metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar with current learning rate and weight decay\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{current_lr:.2e}',\n",
    "            'wd': f'{current_weight_decay:.2e}'\n",
    "        })\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'learning_rate': scheduler.get_last_lr()[0],\n",
    "        'weight_decay': current_weight_decay\n",
    "    }\n",
    "\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics including accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(model, train_data, train_labels, val_data, val_labels, fold):\n",
    "    # Create datasets and dataloaders\n",
    "    model = model.to(device)\n",
    "    clear_memory()\n",
    "    train_dataset = TextDataset(train_data, train_labels, tokenizer, config[\"max_length\"])\n",
    "    val_dataset = TextDataset(val_data, val_labels, tokenizer, config[\"max_length\"])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # Calculate total steps and warmup steps\n",
    "    num_training_steps = len(train_loader) * config['num_epochs']\n",
    "    num_warmup_steps = int(num_training_steps * config['warmup_ratio'])\n",
    "    \n",
    "    # Initialize optimizer with initial learning rate and weight decay\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['initial_learning_rate'],\n",
    "        weight_decay=config['initial_weight_decay'],\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "   \n",
    "    \n",
    "    # Initialize learning rate scheduler with warmup and cosine decay\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    early_stopping_counter = 0\n",
    "    train_metrics_history = []\n",
    "    val_metrics_history = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Fold {fold}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total steps: {num_training_steps}, Warmup steps: {num_warmup_steps}\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Training\n",
    "        train_metrics = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device, \n",
    "            epoch, config['num_epochs']\n",
    "        )\n",
    "        train_metrics_history.append(train_metrics)\n",
    "        clear_memory()\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate_epoch(model, val_loader, device)\n",
    "        val_metrics_history.append(val_metrics)\n",
    "        clear_memory()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nTraining Metrics:\")\n",
    "        print(f\"Loss: {train_metrics['loss']:.4f}\")\n",
    "        print(f\"Precision: {train_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {train_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {train_metrics['f1']:.4f}\")\n",
    "        print(f\"Learning Rate: {train_metrics['learning_rate']:.2e}\")\n",
    "        print(f\"Weight Decay: {train_metrics['weight_decay']:.2e}\")\n",
    "        \n",
    "        print(f\"\\nValidation Metrics:\")\n",
    "        print(f\"Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            early_stopping_counter = 0\n",
    "            best_metrics = val_metrics\n",
    "            \n",
    "            # Save the best model for this fold\n",
    "            os.makedirs(config['output_dir'], exist_ok=True)\n",
    "            model_save_path = os.path.join(config['output_dir'], f'best_model_fold_{fold}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_f1': best_val_f1,\n",
    "                'config': config\n",
    "            }, model_save_path)\n",
    "            print(f\"\\nSaved best model for fold {fold} with F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            \n",
    "        if early_stopping_counter >= config['early_stopping_patience']:\n",
    "            print(\"\\nEarly stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Generate visualizations\n",
    "    generate_combined_plots(\n",
    "        train_metrics_history,\n",
    "        val_metrics_history,\n",
    "        best_metrics['true_labels'],\n",
    "        best_metrics['predictions'],\n",
    "        30,  # number of classes\n",
    "        fold\n",
    "    )\n",
    "    \n",
    "    # Save fold metrics\n",
    "    precision_avg, recall_avg, f1_avg = save_fold_metrics(\n",
    "        best_metrics['true_labels'],\n",
    "        best_metrics['predictions'],\n",
    "        30,\n",
    "        fold\n",
    "    )\n",
    "    \n",
    "    return best_metrics\n",
    "\n",
    "def cross_validate(texts, labels):\n",
    "    global best_f1\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config['num_folds'], shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        print(f\"\\nFold {fold + 1}/{config['num_folds']}\")\n",
    "        print(f\"Train size: {len(train_idx)}, Validation size: {len(val_idx)}\")\n",
    "        clear_memory()\n",
    "        \n",
    "        # Reset model for each fold\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config[\"model_name\"],\n",
    "            config=config_model\n",
    "        ).to(device)\n",
    "        \n",
    "        # Split data\n",
    "        X_train = [texts[i] for i in train_idx]\n",
    "        y_train = [labels[i] for i in train_idx]\n",
    "        X_val = [texts[i] for i in val_idx]\n",
    "        y_val = [labels[i] for i in val_idx]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        metrics = train_and_evaluate(\n",
    "            model, X_train, y_train, X_val, y_val, fold + 1\n",
    "        )\n",
    "        \n",
    "        fold_metrics.append({\n",
    "            'fold': fold + 1,\n",
    "            'loss': metrics['loss'],\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1': metrics['f1']\n",
    "        })\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold + 1} Results:\")\n",
    "        print(f\"Loss: {metrics['loss']:.4f}\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
    "        \n",
    "        del model\n",
    "        clear_memory()\n",
    "    \n",
    "    # Calculate average metrics across folds\n",
    "    avg_metrics = {\n",
    "        'loss': np.mean([m['loss'] for m in fold_metrics]),\n",
    "        'accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'recall': np.mean([m['recall'] for m in fold_metrics]),\n",
    "        'f1': np.mean([m['f1'] for m in fold_metrics])\n",
    "    }\n",
    "    \n",
    "    print('\\nAverage metrics across folds:')\n",
    "    print(f\"Loss: {avg_metrics['loss']:.4f}\")\n",
    "    print(f\"Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {avg_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {avg_metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {avg_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save overall results\n",
    "    results_df = pd.DataFrame(fold_metrics)\n",
    "    results_df.to_csv('fold_results.csv', index=False)\n",
    "    print(\"\\nSaved detailed fold results to 'fold_results.csv'\")\n",
    "    \n",
    "    best_f1 = avg_metrics['f1']\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Generate Overall Results ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_results(fold_metrics_dir, output_filename=\"overall_performance_metrics.csv\"):\n",
    "    # Gather all fold-level CSV files\n",
    "    fold_files = glob.glob(os.path.join(fold_metrics_dir, \"performance_metrics_fold_*.csv\"))\n",
    "    if not fold_files:\n",
    "        raise FileNotFoundError(\"No fold-level performance metrics files found in the directory.\")\n",
    "\n",
    "    # Initialize DataFrame for aggregation\n",
    "    all_folds_metrics = []\n",
    "\n",
    "    # Process each fold file\n",
    "    for file in fold_files:\n",
    "        fold_df = pd.read_csv(file)\n",
    "        all_folds_metrics.append(fold_df)\n",
    "\n",
    "    # Combine all fold data\n",
    "    combined_df = pd.concat(all_folds_metrics)\n",
    "\n",
    "    # Exclude the \"Average\" row for class-level aggregation\n",
    "    class_only_df = combined_df[~combined_df[\"Class\"].str.contains(\"Average\")]\n",
    "\n",
    "    # Aggregate metrics by class\n",
    "    aggregated_metrics = class_only_df.groupby(\"Class\").mean().reset_index()\n",
    "\n",
    "    # Sort the metrics by Class\n",
    "    aggregated_metrics[\"Class\"] = aggregated_metrics[\"Class\"].str.extract(r'(\\d+)').astype(int)\n",
    "    aggregated_metrics = aggregated_metrics.sort_values(by=\"Class\").reset_index(drop=True)\n",
    "\n",
    "    # Compute overall averages\n",
    "    overall_precision = aggregated_metrics[\"Precision\"].mean()\n",
    "    overall_recall = aggregated_metrics[\"Recall\"].mean()\n",
    "    overall_f1 = aggregated_metrics[\"F1-Score\"].mean()\n",
    "\n",
    "    # Add \"Average\" row to the results using pd.concat\n",
    "    average_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Class\": [\"Average\"],\n",
    "            \"Precision\": [overall_precision],\n",
    "            \"Recall\": [overall_recall],\n",
    "            \"F1-Score\": [overall_f1],\n",
    "        }\n",
    "    )\n",
    "    aggregated_metrics = pd.concat([aggregated_metrics, average_row], ignore_index=True)\n",
    "\n",
    "    # Save to a new CSV file\n",
    "    aggregated_metrics.to_csv(output_filename, index=False)\n",
    "    print(f\"Overall performance metrics saved to '{output_filename}'.\")\n",
    "\n",
    "    return aggregated_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_as_dataframe(aggregated_metrics):\n",
    "    \"\"\"\n",
    "    Display the overall performance metrics as a clean DataFrame.\n",
    "\n",
    "    Args:\n",
    "        aggregated_metrics (pd.DataFrame): DataFrame containing overall performance metrics.\n",
    "    \"\"\"\n",
    "    # Rename columns to match the teacher's format\n",
    "    aggregated_metrics = aggregated_metrics.rename(columns={\"Class\": \" \", \"Precision\": \"Precision\", \"Recall\": \"Recall\", \"F1-Score\": \"F-Score\"})\n",
    "    \n",
    "    # Display the DataFrame as is\n",
    "    display(aggregated_metrics.style.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n",
    "         {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
    "    ).set_caption(\"Overall Performance Metrics\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Execute Training ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Train size: 1200, Validation size: 300\n",
      "\n",
      "==================================================\n",
      "Training Fold 1\n",
      "==================================================\n",
      "Total steps: 3000, Warmup steps: 600\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:15<00:00,  1.53batch/s, loss=3.1676, lr=1.50e-05, wd=1.00e-02]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 3.4507\n",
      "Precision: 0.0296\n",
      "Recall: 0.0383\n",
      "F1-Score: 0.0292\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 1.00e-02\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 3.2467\n",
      "Accuracy: 0.1300\n",
      "Precision: 0.0611\n",
      "Recall: 0.1300\n",
      "F1-Score: 0.0643\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.0643\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=2.9511, lr=3.00e-05, wd=9.10e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 2.8286\n",
      "Precision: 0.2630\n",
      "Recall: 0.2567\n",
      "F1-Score: 0.2354\n",
      "Learning Rate: 3.00e-05\n",
      "Weight Decay: 9.10e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 2.0417\n",
      "Accuracy: 0.5167\n",
      "Precision: 0.5008\n",
      "Recall: 0.5167\n",
      "F1-Score: 0.4495\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.4495\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:14<00:00,  1.54batch/s, loss=1.2142, lr=2.89e-05, wd=8.20e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.6600\n",
      "Precision: 0.6902\n",
      "Recall: 0.6817\n",
      "F1-Score: 0.6733\n",
      "Learning Rate: 2.89e-05\n",
      "Weight Decay: 8.20e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.2702\n",
      "Accuracy: 0.7900\n",
      "Precision: 0.8232\n",
      "Recall: 0.7900\n",
      "F1-Score: 0.7696\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.7696\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.8007, lr=2.56e-05, wd=7.30e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.0612\n",
      "Precision: 0.8604\n",
      "Recall: 0.8625\n",
      "F1-Score: 0.8594\n",
      "Learning Rate: 2.56e-05\n",
      "Weight Decay: 7.30e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0495\n",
      "Accuracy: 0.8467\n",
      "Precision: 0.8659\n",
      "Recall: 0.8467\n",
      "F1-Score: 0.8418\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.8418\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:08<00:00,  1.59batch/s, loss=0.6609, lr=2.07e-05, wd=6.40e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.8158\n",
      "Precision: 0.9493\n",
      "Recall: 0.9492\n",
      "F1-Score: 0.9487\n",
      "Learning Rate: 2.07e-05\n",
      "Weight Decay: 6.40e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9092\n",
      "Accuracy: 0.9033\n",
      "Precision: 0.9187\n",
      "Recall: 0.9033\n",
      "F1-Score: 0.9049\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.9049\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.6516, lr=1.50e-05, wd=5.50e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.7083\n",
      "Precision: 0.9827\n",
      "Recall: 0.9825\n",
      "F1-Score: 0.9825\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 5.50e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9405\n",
      "Accuracy: 0.9033\n",
      "Precision: 0.9237\n",
      "Recall: 0.9033\n",
      "F1-Score: 0.8991\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:13<00:00,  1.55batch/s, loss=0.6579, lr=9.26e-06, wd=4.60e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6671\n",
      "Precision: 0.9967\n",
      "Recall: 0.9967\n",
      "F1-Score: 0.9967\n",
      "Learning Rate: 9.26e-06\n",
      "Weight Decay: 4.60e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8519\n",
      "Accuracy: 0.9267\n",
      "Precision: 0.9360\n",
      "Recall: 0.9267\n",
      "F1-Score: 0.9271\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.9271\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.6487, lr=4.39e-06, wd=3.70e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6591\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 4.39e-06\n",
      "Weight Decay: 3.70e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8292\n",
      "Accuracy: 0.9367\n",
      "Precision: 0.9401\n",
      "Recall: 0.9367\n",
      "F1-Score: 0.9362\n",
      "\n",
      "Saved best model for fold 1 with F1: 0.9362\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:15<00:00,  1.53batch/s, loss=0.6562, lr=1.14e-06, wd=2.80e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6564\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 1.14e-06\n",
      "Weight Decay: 2.80e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8367\n",
      "Accuracy: 0.9333\n",
      "Precision: 0.9398\n",
      "Recall: 0.9333\n",
      "F1-Score: 0.9337\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:14<00:00,  1.54batch/s, loss=0.6509, lr=0.00e+00, wd=1.90e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6555\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "F1-Score: 0.9992\n",
      "Learning Rate: 0.00e+00\n",
      "Weight Decay: 1.90e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8402\n",
      "Accuracy: 0.9300\n",
      "Precision: 0.9384\n",
      "Recall: 0.9300\n",
      "F1-Score: 0.9305\n",
      "Confusion Matrix Shape: (30, 30)\n",
      "Plots saved to plots/dbmdz_bert-base-turkish-cased/combined_metrics_fold_1.png\n",
      "Metrics for fold 1 saved to fold_metrics/dbmdz_bert-base-turkish-cased/performance_metrics_fold_1.csv\n",
      "\n",
      "Fold 1 Results:\n",
      "Loss: 0.8292\n",
      "Accuracy: 0.9367\n",
      "Precision: 0.9401\n",
      "Recall: 0.9367\n",
      "F1-Score: 0.9362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2/5\n",
      "Train size: 1200, Validation size: 300\n",
      "\n",
      "==================================================\n",
      "Training Fold 2\n",
      "==================================================\n",
      "Total steps: 3000, Warmup steps: 600\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:15<00:00,  1.54batch/s, loss=3.1956, lr=1.50e-05, wd=1.00e-02]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 3.4176\n",
      "Precision: 0.0463\n",
      "Recall: 0.0450\n",
      "F1-Score: 0.0389\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 1.00e-02\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 3.1758\n",
      "Accuracy: 0.1500\n",
      "Precision: 0.1162\n",
      "Recall: 0.1500\n",
      "F1-Score: 0.0789\n",
      "\n",
      "Saved best model for fold 2 with F1: 0.0789\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:11<00:00,  1.56batch/s, loss=1.7823, lr=3.00e-05, wd=9.10e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 2.7445\n",
      "Precision: 0.2868\n",
      "Recall: 0.2933\n",
      "F1-Score: 0.2745\n",
      "Learning Rate: 3.00e-05\n",
      "Weight Decay: 9.10e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.9456\n",
      "Accuracy: 0.5600\n",
      "Precision: 0.5678\n",
      "Recall: 0.5600\n",
      "F1-Score: 0.5093\n",
      "\n",
      "Saved best model for fold 2 with F1: 0.5093\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:13<00:00,  1.55batch/s, loss=0.9900, lr=2.89e-05, wd=8.20e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.6017\n",
      "Precision: 0.6957\n",
      "Recall: 0.6933\n",
      "F1-Score: 0.6812\n",
      "Learning Rate: 2.89e-05\n",
      "Weight Decay: 8.20e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.3342\n",
      "Accuracy: 0.7467\n",
      "Precision: 0.7727\n",
      "Recall: 0.7467\n",
      "F1-Score: 0.7334\n",
      "\n",
      "Saved best model for fold 2 with F1: 0.7334\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=1.2859, lr=2.56e-05, wd=7.30e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.0549\n",
      "Precision: 0.8653\n",
      "Recall: 0.8658\n",
      "F1-Score: 0.8637\n",
      "Learning Rate: 2.56e-05\n",
      "Weight Decay: 7.30e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.1452\n",
      "Accuracy: 0.8267\n",
      "Precision: 0.8815\n",
      "Recall: 0.8267\n",
      "F1-Score: 0.8196\n",
      "\n",
      "Saved best model for fold 2 with F1: 0.8196\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=1.4795, lr=2.07e-05, wd=6.40e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.8014\n",
      "Precision: 0.9571\n",
      "Recall: 0.9558\n",
      "F1-Score: 0.9555\n",
      "Learning Rate: 2.07e-05\n",
      "Weight Decay: 6.40e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0435\n",
      "Accuracy: 0.8500\n",
      "Precision: 0.8707\n",
      "Recall: 0.8500\n",
      "F1-Score: 0.8470\n",
      "\n",
      "Saved best model for fold 2 with F1: 0.8470\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.6541, lr=1.50e-05, wd=5.50e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6977\n",
      "Precision: 0.9878\n",
      "Recall: 0.9875\n",
      "F1-Score: 0.9875\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 5.50e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0166\n",
      "Accuracy: 0.8800\n",
      "Precision: 0.8966\n",
      "Recall: 0.8800\n",
      "F1-Score: 0.8760\n",
      "\n",
      "Saved best model for fold 2 with F1: 0.8760\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.7063, lr=9.26e-06, wd=4.60e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6715\n",
      "Precision: 0.9935\n",
      "Recall: 0.9933\n",
      "F1-Score: 0.9933\n",
      "Learning Rate: 9.26e-06\n",
      "Weight Decay: 4.60e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9852\n",
      "Accuracy: 0.8900\n",
      "Precision: 0.9074\n",
      "Recall: 0.8900\n",
      "F1-Score: 0.8875\n",
      "\n",
      "Saved best model for fold 2 with F1: 0.8875\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:11<00:00,  1.57batch/s, loss=0.7393, lr=4.39e-06, wd=3.70e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6595\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 4.39e-06\n",
      "Weight Decay: 3.70e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0057\n",
      "Accuracy: 0.8667\n",
      "Precision: 0.8852\n",
      "Recall: 0.8667\n",
      "F1-Score: 0.8651\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:13<00:00,  1.55batch/s, loss=0.6572, lr=1.14e-06, wd=2.80e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6570\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 1.14e-06\n",
      "Weight Decay: 2.80e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9973\n",
      "Accuracy: 0.8800\n",
      "Precision: 0.8987\n",
      "Recall: 0.8800\n",
      "F1-Score: 0.8788\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.6518, lr=0.00e+00, wd=1.90e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6542\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "F1-Score: 0.9992\n",
      "Learning Rate: 0.00e+00\n",
      "Weight Decay: 1.90e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0130\n",
      "Accuracy: 0.8733\n",
      "Precision: 0.8939\n",
      "Recall: 0.8733\n",
      "F1-Score: 0.8721\n",
      "\n",
      "Early stopping triggered\n",
      "Confusion Matrix Shape: (30, 30)\n",
      "Plots saved to plots/dbmdz_bert-base-turkish-cased/combined_metrics_fold_2.png\n",
      "Metrics for fold 2 saved to fold_metrics/dbmdz_bert-base-turkish-cased/performance_metrics_fold_2.csv\n",
      "\n",
      "Fold 2 Results:\n",
      "Loss: 0.9852\n",
      "Accuracy: 0.8900\n",
      "Precision: 0.9074\n",
      "Recall: 0.8900\n",
      "F1-Score: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3/5\n",
      "Train size: 1200, Validation size: 300\n",
      "\n",
      "==================================================\n",
      "Training Fold 3\n",
      "==================================================\n",
      "Total steps: 3000, Warmup steps: 600\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:11<00:00,  1.56batch/s, loss=3.4857, lr=1.50e-05, wd=1.00e-02]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 3.4452\n",
      "Precision: 0.0628\n",
      "Recall: 0.0475\n",
      "F1-Score: 0.0344\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 1.00e-02\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 3.3365\n",
      "Accuracy: 0.1333\n",
      "Precision: 0.0446\n",
      "Recall: 0.1333\n",
      "F1-Score: 0.0602\n",
      "\n",
      "Saved best model for fold 3 with F1: 0.0602\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.58batch/s, loss=1.9697, lr=3.00e-05, wd=9.10e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 2.8184\n",
      "Precision: 0.2852\n",
      "Recall: 0.2683\n",
      "F1-Score: 0.2600\n",
      "Learning Rate: 3.00e-05\n",
      "Weight Decay: 9.10e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.9860\n",
      "Accuracy: 0.5667\n",
      "Precision: 0.5794\n",
      "Recall: 0.5667\n",
      "F1-Score: 0.5166\n",
      "\n",
      "Saved best model for fold 3 with F1: 0.5166\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=1.1848, lr=2.89e-05, wd=8.20e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.6521\n",
      "Precision: 0.6637\n",
      "Recall: 0.6667\n",
      "F1-Score: 0.6550\n",
      "Learning Rate: 2.89e-05\n",
      "Weight Decay: 8.20e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.1419\n",
      "Accuracy: 0.8567\n",
      "Precision: 0.8873\n",
      "Recall: 0.8567\n",
      "F1-Score: 0.8478\n",
      "\n",
      "Saved best model for fold 3 with F1: 0.8478\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.59batch/s, loss=1.5226, lr=2.56e-05, wd=7.30e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.0300\n",
      "Precision: 0.8867\n",
      "Recall: 0.8858\n",
      "F1-Score: 0.8841\n",
      "Learning Rate: 2.56e-05\n",
      "Weight Decay: 7.30e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9521\n",
      "Accuracy: 0.8733\n",
      "Precision: 0.8946\n",
      "Recall: 0.8733\n",
      "F1-Score: 0.8736\n",
      "\n",
      "Saved best model for fold 3 with F1: 0.8736\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6742, lr=2.07e-05, wd=6.40e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.7995\n",
      "Precision: 0.9490\n",
      "Recall: 0.9483\n",
      "F1-Score: 0.9480\n",
      "Learning Rate: 2.07e-05\n",
      "Weight Decay: 6.40e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9588\n",
      "Accuracy: 0.8867\n",
      "Precision: 0.9090\n",
      "Recall: 0.8867\n",
      "F1-Score: 0.8800\n",
      "\n",
      "Saved best model for fold 3 with F1: 0.8800\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6528, lr=1.50e-05, wd=5.50e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6997\n",
      "Precision: 0.9845\n",
      "Recall: 0.9842\n",
      "F1-Score: 0.9841\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 5.50e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9091\n",
      "Accuracy: 0.9067\n",
      "Precision: 0.9180\n",
      "Recall: 0.9067\n",
      "F1-Score: 0.9045\n",
      "\n",
      "Saved best model for fold 3 with F1: 0.9045\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.58batch/s, loss=0.6524, lr=9.26e-06, wd=4.60e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6688\n",
      "Precision: 0.9960\n",
      "Recall: 0.9958\n",
      "F1-Score: 0.9958\n",
      "Learning Rate: 9.26e-06\n",
      "Weight Decay: 4.60e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9062\n",
      "Accuracy: 0.9167\n",
      "Precision: 0.9262\n",
      "Recall: 0.9167\n",
      "F1-Score: 0.9140\n",
      "\n",
      "Saved best model for fold 3 with F1: 0.9140\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6514, lr=4.39e-06, wd=3.70e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6582\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "F1-Score: 0.9992\n",
      "Learning Rate: 4.39e-06\n",
      "Weight Decay: 3.70e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9057\n",
      "Accuracy: 0.9067\n",
      "Precision: 0.9212\n",
      "Recall: 0.9067\n",
      "F1-Score: 0.9034\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.6506, lr=1.14e-06, wd=2.80e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6546\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Learning Rate: 1.14e-06\n",
      "Weight Decay: 2.80e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8821\n",
      "Accuracy: 0.9067\n",
      "Precision: 0.9176\n",
      "Recall: 0.9067\n",
      "F1-Score: 0.9042\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6535, lr=0.00e+00, wd=1.90e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6543\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "F1-Score: 0.9992\n",
      "Learning Rate: 0.00e+00\n",
      "Weight Decay: 1.90e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8830\n",
      "Accuracy: 0.9100\n",
      "Precision: 0.9218\n",
      "Recall: 0.9100\n",
      "F1-Score: 0.9075\n",
      "\n",
      "Early stopping triggered\n",
      "Confusion Matrix Shape: (30, 30)\n",
      "Plots saved to plots/dbmdz_bert-base-turkish-cased/combined_metrics_fold_3.png\n",
      "Metrics for fold 3 saved to fold_metrics/dbmdz_bert-base-turkish-cased/performance_metrics_fold_3.csv\n",
      "\n",
      "Fold 3 Results:\n",
      "Loss: 0.9062\n",
      "Accuracy: 0.9167\n",
      "Precision: 0.9262\n",
      "Recall: 0.9167\n",
      "F1-Score: 0.9140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4/5\n",
      "Train size: 1200, Validation size: 300\n",
      "\n",
      "==================================================\n",
      "Training Fold 4\n",
      "==================================================\n",
      "Total steps: 3000, Warmup steps: 600\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=3.1255, lr=1.50e-05, wd=1.00e-02]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 3.4169\n",
      "Precision: 0.0837\n",
      "Recall: 0.0575\n",
      "F1-Score: 0.0451\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 1.00e-02\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 3.1952\n",
      "Accuracy: 0.1700\n",
      "Precision: 0.1940\n",
      "Recall: 0.1700\n",
      "F1-Score: 0.1309\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.1309\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.57batch/s, loss=2.6366, lr=3.00e-05, wd=9.10e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 2.6970\n",
      "Precision: 0.2874\n",
      "Recall: 0.2950\n",
      "F1-Score: 0.2727\n",
      "Learning Rate: 3.00e-05\n",
      "Weight Decay: 9.10e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.9219\n",
      "Accuracy: 0.5867\n",
      "Precision: 0.6467\n",
      "Recall: 0.5867\n",
      "F1-Score: 0.5282\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.5282\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.57batch/s, loss=1.0758, lr=2.89e-05, wd=8.20e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.5812\n",
      "Precision: 0.7079\n",
      "Recall: 0.7117\n",
      "F1-Score: 0.7045\n",
      "Learning Rate: 2.89e-05\n",
      "Weight Decay: 8.20e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.2756\n",
      "Accuracy: 0.7700\n",
      "Precision: 0.8212\n",
      "Recall: 0.7700\n",
      "F1-Score: 0.7622\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.7622\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.57batch/s, loss=0.7996, lr=2.56e-05, wd=7.30e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.0016\n",
      "Precision: 0.8899\n",
      "Recall: 0.8908\n",
      "F1-Score: 0.8892\n",
      "Learning Rate: 2.56e-05\n",
      "Weight Decay: 7.30e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0341\n",
      "Accuracy: 0.8733\n",
      "Precision: 0.8809\n",
      "Recall: 0.8733\n",
      "F1-Score: 0.8707\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.8707\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.59batch/s, loss=0.6777, lr=2.07e-05, wd=6.40e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.7842\n",
      "Precision: 0.9595\n",
      "Recall: 0.9592\n",
      "F1-Score: 0.9590\n",
      "Learning Rate: 2.07e-05\n",
      "Weight Decay: 6.40e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0484\n",
      "Accuracy: 0.8733\n",
      "Precision: 0.8864\n",
      "Recall: 0.8733\n",
      "F1-Score: 0.8688\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.58batch/s, loss=0.6572, lr=1.50e-05, wd=5.50e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6941\n",
      "Precision: 0.9879\n",
      "Recall: 0.9875\n",
      "F1-Score: 0.9875\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 5.50e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0046\n",
      "Accuracy: 0.8833\n",
      "Precision: 0.8949\n",
      "Recall: 0.8833\n",
      "F1-Score: 0.8798\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.8798\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6506, lr=9.26e-06, wd=4.60e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6721\n",
      "Precision: 0.9942\n",
      "Recall: 0.9942\n",
      "F1-Score: 0.9941\n",
      "Learning Rate: 9.26e-06\n",
      "Weight Decay: 4.60e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9655\n",
      "Accuracy: 0.9000\n",
      "Precision: 0.9144\n",
      "Recall: 0.9000\n",
      "F1-Score: 0.8975\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.8975\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:08<00:00,  1.59batch/s, loss=0.6509, lr=4.39e-06, wd=3.70e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6596\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 4.39e-06\n",
      "Weight Decay: 3.70e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9396\n",
      "Accuracy: 0.9067\n",
      "Precision: 0.9192\n",
      "Recall: 0.9067\n",
      "F1-Score: 0.9052\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.9052\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6534, lr=1.14e-06, wd=2.80e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6558\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "F1-Score: 0.9992\n",
      "Learning Rate: 1.14e-06\n",
      "Weight Decay: 2.80e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9043\n",
      "Accuracy: 0.9233\n",
      "Precision: 0.9293\n",
      "Recall: 0.9233\n",
      "F1-Score: 0.9227\n",
      "\n",
      "Saved best model for fold 4 with F1: 0.9227\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.58batch/s, loss=0.6504, lr=0.00e+00, wd=1.90e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6557\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 0.00e+00\n",
      "Weight Decay: 1.90e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9071\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9277\n",
      "Recall: 0.9200\n",
      "F1-Score: 0.9194\n",
      "Confusion Matrix Shape: (30, 30)\n",
      "Plots saved to plots/dbmdz_bert-base-turkish-cased/combined_metrics_fold_4.png\n",
      "Metrics for fold 4 saved to fold_metrics/dbmdz_bert-base-turkish-cased/performance_metrics_fold_4.csv\n",
      "\n",
      "Fold 4 Results:\n",
      "Loss: 0.9043\n",
      "Accuracy: 0.9233\n",
      "Precision: 0.9293\n",
      "Recall: 0.9233\n",
      "F1-Score: 0.9227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5/5\n",
      "Train size: 1200, Validation size: 300\n",
      "\n",
      "==================================================\n",
      "Training Fold 5\n",
      "==================================================\n",
      "Total steps: 3000, Warmup steps: 600\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.57batch/s, loss=3.2787, lr=1.50e-05, wd=1.00e-02]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 3.4687\n",
      "Precision: 0.0202\n",
      "Recall: 0.0325\n",
      "F1-Score: 0.0209\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 1.00e-02\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 3.3496\n",
      "Accuracy: 0.0833\n",
      "Precision: 0.0354\n",
      "Recall: 0.0833\n",
      "F1-Score: 0.0411\n",
      "\n",
      "Saved best model for fold 5 with F1: 0.0411\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.57batch/s, loss=1.6988, lr=3.00e-05, wd=9.10e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 2.8808\n",
      "Precision: 0.2833\n",
      "Recall: 0.2625\n",
      "F1-Score: 0.2526\n",
      "Learning Rate: 3.00e-05\n",
      "Weight Decay: 9.10e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 2.1217\n",
      "Accuracy: 0.5033\n",
      "Precision: 0.6007\n",
      "Recall: 0.5033\n",
      "F1-Score: 0.4669\n",
      "\n",
      "Saved best model for fold 5 with F1: 0.4669\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:11<00:00,  1.56batch/s, loss=0.9219, lr=2.89e-05, wd=8.20e-03]\n",
      "/Users/egebilge/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.6106\n",
      "Precision: 0.6963\n",
      "Recall: 0.6958\n",
      "F1-Score: 0.6880\n",
      "Learning Rate: 2.89e-05\n",
      "Weight Decay: 8.20e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.1966\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.8001\n",
      "Recall: 0.8100\n",
      "F1-Score: 0.7835\n",
      "\n",
      "Saved best model for fold 5 with F1: 0.7835\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:11<00:00,  1.57batch/s, loss=0.9533, lr=2.56e-05, wd=7.30e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 1.0083\n",
      "Precision: 0.8764\n",
      "Recall: 0.8783\n",
      "F1-Score: 0.8760\n",
      "Learning Rate: 2.56e-05\n",
      "Weight Decay: 7.30e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9181\n",
      "Accuracy: 0.9033\n",
      "Precision: 0.9114\n",
      "Recall: 0.9033\n",
      "F1-Score: 0.9003\n",
      "\n",
      "Saved best model for fold 5 with F1: 0.9003\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:12<00:00,  1.56batch/s, loss=0.6729, lr=2.07e-05, wd=6.40e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.7806\n",
      "Precision: 0.9596\n",
      "Recall: 0.9592\n",
      "F1-Score: 0.9590\n",
      "Learning Rate: 2.07e-05\n",
      "Weight Decay: 6.40e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.9336\n",
      "Accuracy: 0.8900\n",
      "Precision: 0.9051\n",
      "Recall: 0.8900\n",
      "F1-Score: 0.8858\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:10<00:00,  1.58batch/s, loss=0.6513, lr=1.50e-05, wd=5.50e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6928\n",
      "Precision: 0.9853\n",
      "Recall: 0.9850\n",
      "F1-Score: 0.9850\n",
      "Learning Rate: 1.50e-05\n",
      "Weight Decay: 5.50e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8715\n",
      "Accuracy: 0.9233\n",
      "Precision: 0.9286\n",
      "Recall: 0.9233\n",
      "F1-Score: 0.9202\n",
      "\n",
      "Saved best model for fold 5 with F1: 0.9202\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:08<00:00,  1.59batch/s, loss=0.6555, lr=9.26e-06, wd=4.60e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6622\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 9.26e-06\n",
      "Weight Decay: 4.60e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8598\n",
      "Accuracy: 0.9300\n",
      "Precision: 0.9343\n",
      "Recall: 0.9300\n",
      "F1-Score: 0.9277\n",
      "\n",
      "Saved best model for fold 5 with F1: 0.9277\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6544, lr=4.39e-06, wd=3.70e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6565\n",
      "Precision: 0.9984\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.9983\n",
      "Learning Rate: 4.39e-06\n",
      "Weight Decay: 3.70e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8713\n",
      "Accuracy: 0.9267\n",
      "Precision: 0.9316\n",
      "Recall: 0.9267\n",
      "F1-Score: 0.9235\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:09<00:00,  1.58batch/s, loss=0.6497, lr=1.14e-06, wd=2.80e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6535\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Learning Rate: 1.14e-06\n",
      "Weight Decay: 2.80e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8896\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9246\n",
      "Recall: 0.9200\n",
      "F1-Score: 0.9150\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 300/300 [03:08<00:00,  1.59batch/s, loss=0.6570, lr=0.00e+00, wd=1.90e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "Loss: 0.6528\n",
      "Precision: 0.9992\n",
      "Recall: 0.9992\n",
      "F1-Score: 0.9992\n",
      "Learning Rate: 0.00e+00\n",
      "Weight Decay: 1.90e-03\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.8884\n",
      "Accuracy: 0.9167\n",
      "Precision: 0.9221\n",
      "Recall: 0.9167\n",
      "F1-Score: 0.9118\n",
      "\n",
      "Early stopping triggered\n",
      "Confusion Matrix Shape: (30, 30)\n",
      "Plots saved to plots/dbmdz_bert-base-turkish-cased/combined_metrics_fold_5.png\n",
      "Metrics for fold 5 saved to fold_metrics/dbmdz_bert-base-turkish-cased/performance_metrics_fold_5.csv\n",
      "\n",
      "Fold 5 Results:\n",
      "Loss: 0.8598\n",
      "Accuracy: 0.9300\n",
      "Precision: 0.9343\n",
      "Recall: 0.9300\n",
      "F1-Score: 0.9277\n",
      "\n",
      "Average metrics across folds:\n",
      "Loss: 0.8969\n",
      "Accuracy: 0.9193\n",
      "Precision: 0.9274\n",
      "Recall: 0.9193\n",
      "F1-Score: 0.9176\n",
      "\n",
      "Saved detailed fold results to 'fold_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Execute Cross-Validation with Hyperparameter Search ---\n",
    "best_metrics = cross_validate(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'best_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Save Overall Results ---\n",
    "results_df = pd.DataFrame([{\n",
    "    'Loss': best_metrics['loss'],\n",
    "    'Accuracy': best_metrics['accuracy'],\n",
    "    'Precision': best_metrics['precision'],\n",
    "    'Recall': best_metrics['recall'],\n",
    "    'F1-Score': best_metrics['f1']\n",
    "}])\n",
    "results_df.to_csv(\"best_metrics.csv\", index=False)\n",
    "print(\"Results saved to 'best_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall performance metrics saved to 'overall_performance_metrics.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b2814 th {\n",
       "  text-align: center;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_b2814 td {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b2814\">\n",
       "  <caption>Overall Performance Metrics</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b2814_level0_col0\" class=\"col_heading level0 col0\" > </th>\n",
       "      <th id=\"T_b2814_level0_col1\" class=\"col_heading level0 col1\" >Precision</th>\n",
       "      <th id=\"T_b2814_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_b2814_level0_col3\" class=\"col_heading level0 col3\" >F-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b2814_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_b2814_row0_col1\" class=\"data row0 col1\" >0.911111</td>\n",
       "      <td id=\"T_b2814_row0_col2\" class=\"data row0 col2\" >0.960000</td>\n",
       "      <td id=\"T_b2814_row0_col3\" class=\"data row0 col3\" >0.932057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b2814_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_b2814_row1_col1\" class=\"data row1 col1\" >0.906667</td>\n",
       "      <td id=\"T_b2814_row1_col2\" class=\"data row1 col2\" >0.920000</td>\n",
       "      <td id=\"T_b2814_row1_col3\" class=\"data row1 col3\" >0.911292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b2814_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_b2814_row2_col1\" class=\"data row2 col1\" >0.856429</td>\n",
       "      <td id=\"T_b2814_row2_col2\" class=\"data row2 col2\" >0.640000</td>\n",
       "      <td id=\"T_b2814_row2_col3\" class=\"data row2 col3\" >0.720224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b2814_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_b2814_row3_col1\" class=\"data row3 col1\" >0.981818</td>\n",
       "      <td id=\"T_b2814_row3_col2\" class=\"data row3 col2\" >0.960000</td>\n",
       "      <td id=\"T_b2814_row3_col3\" class=\"data row3 col3\" >0.969424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b2814_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_b2814_row4_col1\" class=\"data row4 col1\" >0.960000</td>\n",
       "      <td id=\"T_b2814_row4_col2\" class=\"data row4 col2\" >0.880000</td>\n",
       "      <td id=\"T_b2814_row4_col3\" class=\"data row4 col3\" >0.916725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b2814_row5_col0\" class=\"data row5 col0\" >6</td>\n",
       "      <td id=\"T_b2814_row5_col1\" class=\"data row5 col1\" >0.981818</td>\n",
       "      <td id=\"T_b2814_row5_col2\" class=\"data row5 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row5_col3\" class=\"data row5 col3\" >0.990476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b2814_row6_col0\" class=\"data row6 col0\" >7</td>\n",
       "      <td id=\"T_b2814_row6_col1\" class=\"data row6 col1\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row6_col2\" class=\"data row6 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row6_col3\" class=\"data row6 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_b2814_row7_col0\" class=\"data row7 col0\" >8</td>\n",
       "      <td id=\"T_b2814_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row7_col2\" class=\"data row7 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row7_col3\" class=\"data row7 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_b2814_row8_col0\" class=\"data row8 col0\" >9</td>\n",
       "      <td id=\"T_b2814_row8_col1\" class=\"data row8 col1\" >0.905000</td>\n",
       "      <td id=\"T_b2814_row8_col2\" class=\"data row8 col2\" >0.920000</td>\n",
       "      <td id=\"T_b2814_row8_col3\" class=\"data row8 col3\" >0.901098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_b2814_row9_col0\" class=\"data row9 col0\" >10</td>\n",
       "      <td id=\"T_b2814_row9_col1\" class=\"data row9 col1\" >0.852525</td>\n",
       "      <td id=\"T_b2814_row9_col2\" class=\"data row9 col2\" >0.780000</td>\n",
       "      <td id=\"T_b2814_row9_col3\" class=\"data row9 col3\" >0.804637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_b2814_row10_col0\" class=\"data row10 col0\" >11</td>\n",
       "      <td id=\"T_b2814_row10_col1\" class=\"data row10 col1\" >0.930303</td>\n",
       "      <td id=\"T_b2814_row10_col2\" class=\"data row10 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row10_col3\" class=\"data row10 col3\" >0.962771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_b2814_row11_col0\" class=\"data row11 col0\" >12</td>\n",
       "      <td id=\"T_b2814_row11_col1\" class=\"data row11 col1\" >0.896970</td>\n",
       "      <td id=\"T_b2814_row11_col2\" class=\"data row11 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row11_col3\" class=\"data row11 col3\" >0.944589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_b2814_row12_col0\" class=\"data row12 col0\" >13</td>\n",
       "      <td id=\"T_b2814_row12_col1\" class=\"data row12 col1\" >0.963636</td>\n",
       "      <td id=\"T_b2814_row12_col2\" class=\"data row12 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row12_col3\" class=\"data row12 col3\" >0.980952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_b2814_row13_col0\" class=\"data row13 col0\" >14</td>\n",
       "      <td id=\"T_b2814_row13_col1\" class=\"data row13 col1\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row13_col2\" class=\"data row13 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row13_col3\" class=\"data row13 col3\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_b2814_row14_col0\" class=\"data row14 col0\" >15</td>\n",
       "      <td id=\"T_b2814_row14_col1\" class=\"data row14 col1\" >0.766480</td>\n",
       "      <td id=\"T_b2814_row14_col2\" class=\"data row14 col2\" >0.940000</td>\n",
       "      <td id=\"T_b2814_row14_col3\" class=\"data row14 col3\" >0.840207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_b2814_row15_col0\" class=\"data row15 col0\" >16</td>\n",
       "      <td id=\"T_b2814_row15_col1\" class=\"data row15 col1\" >0.891818</td>\n",
       "      <td id=\"T_b2814_row15_col2\" class=\"data row15 col2\" >0.920000</td>\n",
       "      <td id=\"T_b2814_row15_col3\" class=\"data row15 col3\" >0.903586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_b2814_row16_col0\" class=\"data row16 col0\" >17</td>\n",
       "      <td id=\"T_b2814_row16_col1\" class=\"data row16 col1\" >0.963636</td>\n",
       "      <td id=\"T_b2814_row16_col2\" class=\"data row16 col2\" >0.980000</td>\n",
       "      <td id=\"T_b2814_row16_col3\" class=\"data row16 col3\" >0.970426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_b2814_row17_col0\" class=\"data row17 col0\" >18</td>\n",
       "      <td id=\"T_b2814_row17_col1\" class=\"data row17 col1\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row17_col2\" class=\"data row17 col2\" >0.720000</td>\n",
       "      <td id=\"T_b2814_row17_col3\" class=\"data row17 col3\" >0.831957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_b2814_row18_col0\" class=\"data row18 col0\" >19</td>\n",
       "      <td id=\"T_b2814_row18_col1\" class=\"data row18 col1\" >0.918462</td>\n",
       "      <td id=\"T_b2814_row18_col2\" class=\"data row18 col2\" >0.940000</td>\n",
       "      <td id=\"T_b2814_row18_col3\" class=\"data row18 col3\" >0.925995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_b2814_row19_col0\" class=\"data row19 col0\" >20</td>\n",
       "      <td id=\"T_b2814_row19_col1\" class=\"data row19 col1\" >0.892857</td>\n",
       "      <td id=\"T_b2814_row19_col2\" class=\"data row19 col2\" >0.920000</td>\n",
       "      <td id=\"T_b2814_row19_col3\" class=\"data row19 col3\" >0.897554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_b2814_row20_col0\" class=\"data row20 col0\" >21</td>\n",
       "      <td id=\"T_b2814_row20_col1\" class=\"data row20 col1\" >0.924444</td>\n",
       "      <td id=\"T_b2814_row20_col2\" class=\"data row20 col2\" >0.760000</td>\n",
       "      <td id=\"T_b2814_row20_col3\" class=\"data row20 col3\" >0.827601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_b2814_row21_col0\" class=\"data row21 col0\" >22</td>\n",
       "      <td id=\"T_b2814_row21_col1\" class=\"data row21 col1\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row21_col2\" class=\"data row21 col2\" >0.860000</td>\n",
       "      <td id=\"T_b2814_row21_col3\" class=\"data row21 col3\" >0.923977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_b2814_row22_col0\" class=\"data row22 col0\" >23</td>\n",
       "      <td id=\"T_b2814_row22_col1\" class=\"data row22 col1\" >0.830000</td>\n",
       "      <td id=\"T_b2814_row22_col2\" class=\"data row22 col2\" >0.860000</td>\n",
       "      <td id=\"T_b2814_row22_col3\" class=\"data row22 col3\" >0.843636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_b2814_row23_col0\" class=\"data row23 col0\" >24</td>\n",
       "      <td id=\"T_b2814_row23_col1\" class=\"data row23 col1\" >0.981818</td>\n",
       "      <td id=\"T_b2814_row23_col2\" class=\"data row23 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row23_col3\" class=\"data row23 col3\" >0.990476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_b2814_row24_col0\" class=\"data row24 col0\" >25</td>\n",
       "      <td id=\"T_b2814_row24_col1\" class=\"data row24 col1\" >0.948485</td>\n",
       "      <td id=\"T_b2814_row24_col2\" class=\"data row24 col2\" >0.980000</td>\n",
       "      <td id=\"T_b2814_row24_col3\" class=\"data row24 col3\" >0.961768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_b2814_row25_col0\" class=\"data row25 col0\" >26</td>\n",
       "      <td id=\"T_b2814_row25_col1\" class=\"data row25 col1\" >0.981818</td>\n",
       "      <td id=\"T_b2814_row25_col2\" class=\"data row25 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row25_col3\" class=\"data row25 col3\" >0.990476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_b2814_row26_col0\" class=\"data row26 col0\" >27</td>\n",
       "      <td id=\"T_b2814_row26_col1\" class=\"data row26 col1\" >0.832634</td>\n",
       "      <td id=\"T_b2814_row26_col2\" class=\"data row26 col2\" >1.000000</td>\n",
       "      <td id=\"T_b2814_row26_col3\" class=\"data row26 col3\" >0.905342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_b2814_row27_col0\" class=\"data row27 col0\" >28</td>\n",
       "      <td id=\"T_b2814_row27_col1\" class=\"data row27 col1\" >0.893442</td>\n",
       "      <td id=\"T_b2814_row27_col2\" class=\"data row27 col2\" >0.940000</td>\n",
       "      <td id=\"T_b2814_row27_col3\" class=\"data row27 col3\" >0.912810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_b2814_row28_col0\" class=\"data row28 col0\" >29</td>\n",
       "      <td id=\"T_b2814_row28_col1\" class=\"data row28 col1\" >0.895556</td>\n",
       "      <td id=\"T_b2814_row28_col2\" class=\"data row28 col2\" >0.820000</td>\n",
       "      <td id=\"T_b2814_row28_col3\" class=\"data row28 col3\" >0.854620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_b2814_row29_col0\" class=\"data row29 col0\" >30</td>\n",
       "      <td id=\"T_b2814_row29_col1\" class=\"data row29 col1\" >0.955556</td>\n",
       "      <td id=\"T_b2814_row29_col2\" class=\"data row29 col2\" >0.880000</td>\n",
       "      <td id=\"T_b2814_row29_col3\" class=\"data row29 col3\" >0.914620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b2814_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_b2814_row30_col0\" class=\"data row30 col0\" >Average</td>\n",
       "      <td id=\"T_b2814_row30_col1\" class=\"data row30 col1\" >0.927443</td>\n",
       "      <td id=\"T_b2814_row30_col2\" class=\"data row30 col2\" >0.919333</td>\n",
       "      <td id=\"T_b2814_row30_col3\" class=\"data row30 col3\" >0.917643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3d18e7970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate and display the final performance table as required\n",
    "overall_results = generate_overall_results(\"fold_metrics/dbmdz_bert-base-turkish-cased\", output_filename=\"overall_performance_metrics.csv\")\n",
    "display_as_dataframe(overall_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
